# Chapter 21: The Future of Visual Reasoning in Broadcast

When Matthew Davis first showed me Moondream in our R&D lab, I couldn't believe what I was seeing. A model that could understand images, run efficiently, and work with natural language—it felt like science fiction becoming reality.

That was just the beginning.

This book started with a bold claim: cameras can transform from passive recorders to intelligent teammates. Throughout these chapters, you've seen that transformation in action. Now let's look at where this movement is heading—and why you should be part of it.

In this final chapter, I want to share my vision for where visual reasoning in broadcast is heading. Some of this is extrapolation from current trends. Some is speculation based on conversations with researchers and industry insiders. All of it reflects my honest assessment of what's coming.

## The Three-Year Horizon

Let me start with what I believe is achievable in the next three years. This isn't fantasy—it's based on trajectories we can already see.

### Models Get Smaller and Faster

The trend toward efficient models will continue. Moondream proved that you don't need massive infrastructure to run capable visual AI. The next generation will be even smaller, faster, and more capable.

What this means for you:
- Visual reasoning will run on edge devices, not just servers
- Real-time processing (30+ fps) will become practical
- Costs will drop dramatically
- Latency will decrease to imperceptible levels

We're already shipping PTZOptics cameras with powerful processors. Within three years, the AI that processes video might run on the camera itself—no external compute needed.

### Temporal Reasoning Matures

Current visual reasoning models primarily analyze single frames. They can understand what's in an image, but understanding what's happening over time is harder.

The next generation will have native temporal reasoning—understanding video as video, not as a series of disconnected images.

This unlocks:
- True action recognition (not just detection)
- Prediction ("the ball is about to go out of bounds")
- Pattern detection over time ("this speaker tends to pause before making key points")
- Narrative understanding ("this is the climax of the presentation")

Jay and Vik at Moondream are already working on this with their "postage stamp" technique. The future is video-native AI.

### Multimodal Fusion Becomes Standard

Today, combining audio and video AI requires manual integration. Tomorrow, it will be seamless.

Models that natively understand audio, video, and text together will emerge. You won't ask one model about the video and another about the audio—a single model will understand both.

For broadcast, this means:
- True understanding of what's being said AND what's being shown
- Automatic correlation between speaker intent and visual content
- More robust detection (audio can confirm what video suggests)
- Natural conversation interfaces ("make the camera follow whoever is talking")

### Industry-Specific Fine-Tuning

General-purpose models are impressive, but industry-specific models will be transformative.

Imagine a model trained specifically on broadcast production:
- Understands camera angles and shot types
- Knows broadcast terminology
- Recognizes production patterns
- Understands what makes "good TV"

The Visual Reasoning Harness is our contribution to this future—providing the structure and reference material for AI that understands our industry.

## The Ten-Year Horizon

Looking further out requires more speculation, but the trajectory seems clear.

### Autonomous Production

Today, visual reasoning assists human operators. Tomorrow, it might handle entire productions autonomously—for appropriate use cases.

A ten-year-old's basketball game doesn't need a professional production team. A small church service doesn't need a dedicated crew. Corporate all-hands meetings don't need broadcast expertise.

Autonomous production will bring professional-quality video to situations that can't justify professional crews.

But this comes with caveats:
- High-stakes live events will still need humans
- Creative control will remain human
- The AI handles execution; humans handle vision
- Errors will be tolerated differently in different contexts

### Personalized Viewing Experiences

What if every viewer could have their own camera angles? Their own replay access? Their own graphics preferences?

With AI-driven production, this becomes possible. The AI generates multiple perspectives. Viewers choose—or let AI choose for them based on their preferences.

Sports fans might see more replays of their favorite players. Conference attendees might get custom views emphasizing the content most relevant to them. Worshippers might choose how much of the service to see versus the slides.

This is years away, but the direction is clear.

### AV Systems as AI Sensors

This is the vision I keep coming back to: AV systems as the eyes and ears of modern AI.

Every camera, every microphone, every display in a building becomes an AI sensor. The building understands what's happening within it—not to surveil, but to serve.

- Conference rooms configure themselves for the meeting type
- Classrooms adapt to the teaching style
- Worship spaces respond to the spiritual moment
- Event venues anticipate production needs

The technology we're building today is the foundation for this future.

## Emerging Capabilities to Watch

Several emerging capabilities will shape how visual reasoning evolves:

### Embodied AI

Models that understand the physical world—not just images of it—will change what's possible. These models understand physics, spatial relationships, and cause and effect.

For production, this means AI that understands:
- Where the camera could move (not just where it is)
- What movements would create good shots
- How changing one element affects others
- The physical constraints of production

### Reasoning Chains

Current models give answers. Future models will show their work—explicit chains of reasoning that can be inspected and debugged.

"I switched to camera 2 because: (1) the speaker moved to the demo area, (2) camera 2 has the best angle on demo area, (3) the demo is the likely next segment based on slide content, (4) cutting now creates smooth transition during speaker movement."

This makes AI decisions auditable and trustworthy.

### Collaborative AI

Instead of single models making decisions, we'll see teams of AI agents collaborating:
- One agent watches the presenters
- One agent monitors the audience
- One agent manages graphics
- One agent coordinates cameras
- A supervisor agent makes final decisions

Each agent can be specialized and improved independently. The system as a whole is more capable than any single model.

### Real-Time Learning

Today's models are trained offline and deployed static. Future systems will learn continuously from their deployment.

The system that produces your Monday meeting learns from each meeting and improves. By Friday, it's better. By next month, it understands your specific context intimately.

This requires careful handling of privacy and ethics, but the capability is coming.

## How Roles Will Evolve

Visual reasoning won't eliminate jobs in broadcast and AV. But it will change them.

### From Operation to Supervision

Camera operators become AI supervisors. Instead of manually controlling every movement, they set parameters, handle exceptions, and make creative decisions while AI handles routine operation.

This is a skill shift, not a job loss. The best human operators will become the best AI supervisors because they understand what good production looks like.

### From Technical to Creative

When AI handles technical execution, humans can focus more on creative vision.

"I want a dramatic reveal of the keynote speaker" becomes a directive the AI can execute. The human provides creative direction; the AI provides technical implementation.

### From Individual to Systemic

Instead of each crew member managing their element, teams will manage integrated AI systems.

This requires new skills:
- Understanding AI capabilities and limitations
- Configuring AI systems for specific contexts
- Monitoring AI performance
- Intervening effectively when AI fails

### New Roles Emerge

Some roles that don't exist today will be essential:
- AI Production Directors (supervising autonomous production)
- Visual AI Trainers (customizing models for specific use cases)
- AI Ethics Officers (ensuring responsible deployment)
- Human-AI Interaction Designers (creating effective interfaces)

## Preparing for Change

How do you prepare for a future that's uncertain? Here's my advice:

### Build Foundational Skills

The fundamentals don't change:
- Understanding what makes good video
- Knowing your equipment and its capabilities
- Communicating effectively with stakeholders
- Solving problems under pressure

AI tools change. Professional judgment endures.

### Learn to Prompt

Communicating with AI systems is a skill. It's different from programming, different from traditional operation, different from creative direction.

Practice describing visual outcomes in natural language. Learn what level of detail AI needs. Understand how to iterate when results aren't right.

The Visual Reasoning Playground tools are practice for this skill.

### Stay Curious

The technology will keep evolving. Stay curious about new developments. Try new tools. Experiment.

The people who thrive will be those who see change as opportunity, not threat.

### Build Relationships

AI can produce video, but it can't build client relationships. It can't understand unstated needs. It can't navigate organizational politics.

Human skills remain essential. Maybe more essential, as technical skills become commoditized.

### Think Ethically

As AI becomes more capable, ethical considerations become more important.

Build your reputation as someone who deploys AI responsibly. Clients will increasingly want partners who think carefully about these issues.

## The Movement: A Call to Arms

I've said throughout this book that we're not just building a product—we're building a movement. And now it's time to make that explicit.

**This is a movement, not a monopoly.**

Visual reasoning in broadcast and ProAV is bigger than any one company or product. It's a transformation in how our industry works. The question isn't whether AI will change video production—it's who will shape that change.

Will it be closed systems from massive tech companies that lock you into their ecosystems? Or will it be an open community of practitioners who believe in transparency, interoperability, and human empowerment?

We choose the latter. And we're not alone.

**Open ecosystems over closed stacks.** That's why partners like Moondream are building open-weight models anyone can run. That's why LayerJot is creating tools that work with your existing systems, not against them. That's why Detect-IT and MPact Sports are proving that specialized AI can be accessible to organizations of all sizes.

PTZOptics, Moondream, StreamGeeks, LayerJot, Detect-IT, MPact Sports—we're contributors to this movement, not owners of it. The open-source tools, the Visual Reasoning Harness, the educational materials—these belong to everyone.

**Augment people, don't erase them.** The goal has never been to replace broadcast professionals. It's to give you superpowers. To let you focus on the creative decisions that matter while AI handles the repetitive tasks that don't.

My hope is that this book equips you to join the movement. To build things we haven't imagined. To solve problems we don't know about yet. To push the boundaries of what's possible.

**Real outcomes over AI slogans.** We don't care about buzzwords. We care about whether the AI actually helps you produce better content, faster, with less stress.

## Final Thoughts: Your Invitation

Twenty years ago, Matthew Davis and I started building cameras and teaching people how to use them. We had no idea where the journey would lead.

Today, we're at the beginning of another journey—AI transforming what's possible in video production and ProAV.

The technology is ready. The tools are accessible. The opportunity is here.

**But this isn't about the technology. It's about you.**

You're holding this book because you sensed that something important is happening. You're right. And now you have a choice: watch from the sidelines, or step into the arena.

The movement needs practitioners. It needs people who understand both the technology and the craft. People who can bridge the gap between AI capabilities and production realities. People who will build the tools, train the colleagues, and shape the standards.

That's you.

If you've made it this far, you're already ahead of most people in our industry. You understand visual reasoning. You've experimented with the tools. You know what's possible.

Now go build something.

Thank you for reading this book. I hope it's been helpful. I hope it's sparked ideas. And I hope to see what you create.

If you're ever at NAB, IBC, or InfoComm, come find me. I'd love to hear your story.

With all that being said—let's dig in and build the future together.

---

*Paul Richards*
*Chief Streaming Officer, StreamGeeks*
*Downingtown, Pennsylvania*

---

## Continue the Journey

**Visual Reasoning Playground:** Try the tools at VisualReasoning.ai

**GitHub Repository:** Find all code examples at github.com/StreamGeeks/visual-reasoning-playground

**Online Course:** The companion course with hands-on projects is available at StreamGeeks.com

**Our Principles:** Read the full Visual Reasoning Manifesto at https://visualreasoning.ai/our-principles

**Community:** Join the community of builders at VisualReasoning.ai

**Stay Connected:** Follow the latest developments at PTZOptics.com and StreamGeeks.com

---

*This book was written with assistance from AI coding tools—practicing what we preach.*
